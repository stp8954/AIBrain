name: webrag
mode: web
start_loc: https://huggingface.co/docs/peft/index
exclude:
  - https://vespa.ai/pricing
  - https://vespa.ai/sales
  - https://status.vespa.ai/*

# Optional: Web crawling specific parameters
crawl_params:
  respect_robots_txt: true     # Respect robots.txt rules (default: true)
  aggressive_crawl: false       # Enable aggressive crawling (higher speed, more requests) (default: false)
  follow_subdomains: true       # Follow subdomains of the start URL (default: true)
  strict_mode: false            # Only crawl URLs matching the start URL pattern (default: false)
  user_agent_type: chrome       # User agent type: chrome, firefox, safari, mobile, bot (default: chrome)
  # custom_user_agent: "..."    # Custom user agent string (optional, overrides user_agent_type)
  allowed_domains:            # Explicitly allowed domains (optional, auto-detected from start_loc)
    - vespa.ai
    - docs.vespa.ai

# Optional: General parameters for downstream RAG processing
rag_params:
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  embedding_dim: 384              # Embedding dimension (default: 384)
  chunk_size: 1024                # Chunk size for text splitting (default: 1024)
  chunk_overlap: 50
  distance_metric: angular        # Distance metric: angular, euclidean, etc. (default: angular)
  max_tokens: 8192

  # Optional: LangChain-based pipeline configuration
  pipeline:
    chunking_strategy: recursive    # Chunking strategy: fixed, recursive, semantic (default: recursive)
    chunk_size: 1024                # Chunk size for pipeline (overrides above if set)
    chunk_overlap: 50               # Chunk overlap for pipeline

    # Text embedding configuration
    text_embedding:
      model: sentence-transformers/all-MiniLM-L6-v2  # HuggingFace model name
      batch_size: 32                # Batch size for embedding (default: 32)
      device: cpu                   # Device: cpu, cuda, mps (default: cpu)

    # Image embedding configuration (for multimodal search)
    # When enabled, images extracted from PDFs are embedded using CLIP
    # and indexed alongside text for image retrieval in chat responses
    image_embedding:
      enabled: true                 # Enable image embedding (default: true)
      model: sentence-transformers/clip-ViT-B-32  # CLIP model for images (512-dim)
      batch_size: 16                # Batch size for image embedding
      device: cpu                   # Device: cpu, cuda, mps (use cuda for faster processing)
      max_dimension: 1024           # Resize images to max dimension before embedding (default: 1024)

    # Background processing configuration
    # Controls concurrent ingestion jobs and image processing limits
    background:
      max_concurrent_jobs: 2        # Maximum simultaneous ingestion jobs (default: 2)
      enable_image_indexing: true   # Enable image extraction during ingestion (default: true)
      max_images_per_doc: 50        # Maximum images to extract per document (default: 50)

# Optional: Notes and blog generation parameters
notes_params:
  storage_path: ./data/notes       # Directory for notes JSONL storage (default: ./data/notes)
  assets_path: ./data/notes/assets # Directory for uploaded images (default: ./data/notes/assets)
  index_on_save: true              # Index notes in Vespa on save (default: true)

blog_params:
  output_path: ./data/blogs        # Directory for generated blog files (default: ./data/blogs)
  default_template: tutorial       # Default blog template: tutorial, opinion, roundup, technical
  include_sources: true            # Include source citations in generated blogs (default: true)

# =============================================================================
# Image Indexing and Retrieval Configuration
# =============================================================================
#
# NyRAG supports extracting images from documents (PDFs, etc.) and indexing
# them for multimodal search. This allows users to find relevant diagrams,
# charts, and figures based on text queries.
#
# How it works:
# 1. Documents are processed by Docling which extracts embedded images
# 2. Each image is embedded using CLIP (text-image alignment model)
# 3. Images are stored in data/images/{source_id}/{hash}.{format}
# 4. Embeddings are indexed in Vespa alongside text chunks
# 5. Chat queries can retrieve both text and images (include_images=true)
#
# Storage locations:
# - Images: data/images/           (file system)
# - Metadata: nyrag.db             (SQLite image_chunks table)
# - Embeddings: Vespa              (image_embedding field)
#
# API endpoints:
# - GET /api/images/{source_id}/{filename}    Serve image files
# - GET /api/sources/{source_id}/images       List images for a source
# - POST /chat with include_images=true       Retrieve images in responses
#
# Performance considerations:
# - CLIP model loading adds ~2-3s startup time
# - Image embedding is ~100-200ms per image on CPU
# - Use device: cuda for 10x faster embedding on GPU
# - max_images_per_doc limits memory usage for image-heavy documents
